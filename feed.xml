<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://ahrajabi.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://ahrajabi.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-03-28T16:53:24+00:00</updated><id>https://ahrajabi.github.io/feed.xml</id><title type="html">blank</title><subtitle>Personal blog of Amir Rajabi. </subtitle><entry><title type="html">LLMOPT: Democratizing Optimization Through Large Language Models</title><link href="https://ahrajabi.github.io/blog/2025/llmopt_optimisation_llms/" rel="alternate" type="text/html" title="LLMOPT: Democratizing Optimization Through Large Language Models"/><published>2025-03-28T10:00:00+00:00</published><updated>2025-03-28T10:00:00+00:00</updated><id>https://ahrajabi.github.io/blog/2025/llmopt_optimisation_llms</id><content type="html" xml:base="https://ahrajabi.github.io/blog/2025/llmopt_optimisation_llms/"><![CDATA[<p>For years, powerful optimization techniques have remained locked behind complex mathematical notation and specialized programming knowledge—a situation I’ve witnessed firsthand throughout my career in optimization and metaheuristics. Large language models (LLMs) have already revolutionized content creation and programming assistance, but their latest application might be the most transformative yet: making sophisticated optimization accessible to everyone.</p> <p>Enter LLMOPT—a framework that translates everyday business problems into precisely formulated optimization models without requiring specialized expertise. I believe this (besides a few other publications in the past) represents a pivotal moment in how we approach complex decision-making across industries.</p> <h2 id="why-optimization-matters-but-remains-underutilized">Why Optimization Matters (But Remains Underutilized)</h2> <p>Optimization problems are everywhere. Supply chain managers determine optimal inventory levels. Financial analysts find ideal portfolio allocations. Urban planners design efficient transportation networks. These problems share a common structure: determining values for decision variables that maximize or minimize an objective function while satisfying constraints.</p> <p>These diverse problems share the same fundamental structure: finding values for decision variables that optimize an objective while respecting constraints. It’s elegant in theory, but in practice? It’s a mess.</p> <p>The traditional workflow goes something like this:</p> <ol> <li>Translate a real-world problem into mathematical notation (already intimidating)</li> <li>Implement solver code (requiring programming skills most domain experts lack)</li> <li>Verify and interpret results (which often requires debugging complex systems)</li> </ol> <p>No wonder optimization remains underutilized! It’s locked behind steep learning curves in operations research, mathematical modeling, and programming. I’ve watched brilliant professionals resort to spreadsheets and gut instinct because optimization seemed too complex.</p> <h2 id="llmopt-the-solution-idea-ive-been-waiting-for">LLMOPT: The Solution (idea?) I’ve Been Waiting For</h2> <p>When I first read about LLMOPT (Learning to Define and Solve General Optimization Problems from Scratch) in the ICLR 2025 accepted papers, I was genuinely thrilled because I was working on the same thing in my personal project in parallel. This innovative framework addresses exactly what I’ve seen as the major barriers to widespread adoption.</p> <p>LLMOPT tackles what its creators call “optimization generalization” - enhancing both:</p> <ul> <li><strong>Accuracy</strong>: Solving problems correctly across domains</li> <li><strong>Generality</strong>: Handling diverse optimization types, from linear programming to combinatorial optimization</li> </ul> <p>What impresses me most is its structured approach to teaching language models how to formulate and solve optimization problems from natural language descriptions - something I’ve long thought would be a game-changer.</p> <h3 id="five-element-formulation-making-optimization-accessible">Five-Element Formulation: Making Optimization Accessible</h3> <p>The heart of LLMOPT is what they call the “five-element formulation” for defining optimization problems:</p> <ol> <li><strong>Sets</strong>: Definitions of indices and descriptive information</li> <li><strong>Parameters</strong>: Specific numerical values for objective functions and constraints</li> <li><strong>Variables</strong>: Decision variables that need to be optimized</li> <li><strong>Objective</strong>: Function to be minimized or maximized</li> <li><strong>Constraints</strong>: Limitations on variable values</li> </ol> <p>This is not something new, and these elements are already well-defined in the literature. But the question is whether we really need to have the same approach when communicating with LLMs rather than mathematicians (I’ll share my view later in this post).</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/llmopt/llmopt_five_elem_eg-480.webp 480w,/assets/img/blog/llmopt/llmopt_five_elem_eg-800.webp 800w,/assets/img/blog/llmopt/llmopt_five_elem_eg-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/blog/llmopt/llmopt_five_elem_eg.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><em>Figure 2: Example of LLMOPT’s five-element formulation for a resource allocation problem. Source: <a href="https://arxiv.org/abs/2410.13213">LLMOPT: Learning to Define and Solve General Optimization Problems from Scratch</a></em></p> <p>Let me show you what I mean with a resource allocation problem:</p> <blockquote> <p>An accounting firm employs part-time and full-time workers. Full-time workers work 8 hours per shift and are paid $300, while part-time workers work 4 hours per shift and are paid $100. The firm has a project requiring 450 hours of labor with a budget of $15,000. How many of each type of worker should be scheduled to minimize the total number of workers?</p> </blockquote> <p>LLMOPT transforms this into:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>## Sets:
Set of employee types: S = {f, p}

## Parameters:
Hours per shift: hf = 8, hp = 4
Wages per shift: wf = 300, wp = 100
Total labor time required: T = 450
Budget: B = 15000

## Variables:
Employees shifts: xf, xp

## Objective:
Minimize employee number: min xf + xp

## Constraints:
Labor time constraint: hf*xf + hp*xp ≥ T
Budget constraint: wf*xf + wp*xp ≤ B
Positive integer constraint: xf, xp ∈ Z+
</code></pre></div></div> <p>The idea that an LLM could learn to do this automatically feels good but worrying at the same time because they have to be very good at this; otherwise, the cost of wrong formulation is high too. We’ll see how to fix this.</p> <h3 id="advanced-ai-training-beyond-simple-prompting">Advanced AI Training: Beyond Simple Prompting</h3> <p>What differentiates LLMOPT from other approaches I’ve evaluated is that it doesn’t just rely on clever prompting. The multi-instruction supervised fine-tuning on two data types is something I find particularly effective:</p> <ol> <li><strong>Problem-to-formulation</strong>: Learning to define problems using the five-element framework</li> <li><strong>Problem/formulation-to-code</strong>: Learning to generate solver code from either descriptions or formulations</li> </ol> <p>This dual approach reflects how I learned optimization myself - first understanding the abstract formulation, then mastering implementation details.</p> <h3 id="kto-alignment-ensuring-optimization-accuracy">KTO Alignment: Ensuring Optimization Accuracy</h3> <p>One issue I’ve seen with other LLM applications is hallucination - outputs that seem plausible but are incorrect. LLMOPT addresses this through Kahneman-Tversky Optimization (KTO), which uses expert-labeled data to guide model outputs.</p> <p>This approach distinguishes between desirable (correct) and undesirable (incorrect) outputs, which I’ve found critical for complex domains like optimization where errors can propagate in subtle ways.</p> <h3 id="self-correction-automating-the-optimization-debugging-cycle">Self-Correction: Automating the Optimization Debugging Cycle</h3> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/llmopt/llmopt_overview-480.webp 480w,/assets/img/blog/llmopt/llmopt_overview-800.webp 800w,/assets/img/blog/llmopt/llmopt_overview-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/blog/llmopt/llmopt_overview.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><em>Figure 2: Overview of the LLMOPT framework for learning to define and solve optimization problems. Source: <a href="https://arxiv.org/abs/2410.13213">LLMOPT: Learning to Define and Solve General Optimization Problems from Scratch</a></em></p> <p>Perhaps what excites me most about LLMOPT is its self-correction capability. My approach is usually to use an iterative approach to debugging solutions:</p> <ol> <li>Execute code</li> <li>Analyze results for errors</li> <li>Refine and try again</li> </ol> <p>LLMOPT automates this cycle, continuing until an optimal solution is found or a maximum number of iterations is reached.</p> <h2 id="real-world-performance-of-llmopt-across-industries">Real-World Performance of LLMOPT Across Industries</h2> <p>I was genuinely impressed by LLMOPT’s performance across six datasets spanning approximately 20 fields (health, energy, manufacturing, and others) and handling various optimization problem types:</p> <ul> <li>Linear programming</li> <li>Mixed integer programming</li> <li>Nonlinear programming</li> <li>Combinatorial optimization</li> </ul> <p>The authors reported an 11.08% average improvement in solving accuracy compared to existing methods. Is this good still? No clue as long as someone tries it in the real world.</p> <h2 id="enhancing-llmopt-my-proposed-extensions">Enhancing LLMOPT: My Proposed Extensions</h2> <p>While reading through the LLMOPT paper, I couldn’t help but think of several extensions that could make it even more powerful.</p> <h3 id="adding-expressions-as-a-critical-sixth-element">Adding “Expressions” as a Critical Sixth Element</h3> <p>The five-element framework is elegant, but I believe it’s missing a crucial component: “Expressions” as a sixth element. These would be formal definitions encapsulating complex calculations used within constraints or objectives.</p> <p>For example, we can define an expression like this:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>## Expressions:
Transportation_Cost(i,j) = distance(i,j) * cost_per_unit_distance * weight(j)
Carbon_Emissions(i,j) = emissions_factor * fuel_consumption(i,j)
</code></pre></div></div> <p>These can then be referenced in the objective:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>## Objective:
minimize sum(i in WAREHOUSES, j in STORES) Transportation_Cost(i,j) +
         carbon_tax * sum(i in WAREHOUSES, j in STORES) Carbon_Emissions(i,j)
</code></pre></div></div> <p>I’ve found this modular approach offers substantial benefits:</p> <ul> <li>Complex models become much more readable</li> <li>Changes to calculation logic stay isolated</li> <li>Domain experts can verify individual components</li> <li>It’s easier to explain to stakeholders</li> </ul> <h3 id="bridging-theory-and-practice-input-output-formalization">Bridging Theory and Practice: Input-Output Formalization</h3> <p>In my experience, there’s often a gap between mathematical formulation and practical implementation. The five-element approach captures mathematical structure beautifully, but doesn’t address practical implementation details:</p> <ul> <li>What data types should variables use?</li> <li>How should inputs be formatted?</li> <li>What computational trade-offs might be relevant?</li> <li>How should errors be handled?</li> </ul> <p>For instance, the location variables might be x_i in the formulation, but implementation requires decisions about coordinate systems, IDs, or reference objects. This intermediate representation between math and code would make LLMOPT even more robust.</p> <h3 id="combining-self-correction-with-chain-of-thought-reasoning">Combining Self-Correction with Chain of Thought Reasoning</h3> <p>LLMOPT’s execution-based self-correction is powerful, but I wonder if combining it with chain-of-thought (CoT) reasoning might yield even better results. In my own work, I often:</p> <ul> <li>Use reasoning to develop and validate the initial formulation</li> <li>Apply execution-based testing for the final implementation</li> </ul> <p>A hybrid approach could leverage the best of both methods while potentially reducing computation cycles.</p> <h2 id="conclusion-the-future-of-ai-powered-optimization">Conclusion: The Future of AI-Powered Optimization</h2> <p>As I’ve watched AI tools evolve, I’ve become increasingly convinced that LLMOPT-like approaches will transform how professionals work across industries.</p> <p>LLMOPT represents exactly the kind of AI advancement I’ve been hoping for - one that democratizes a complex technical skill rather than simply automating existing processes. By lowering the expertise barrier, it could open optimization’s power to a much wider audience.</p> <p>While there’s room for improvement (as with my suggestions for expressions, input-output formalization, and hybrid reasoning), LLMOPT lays a foundation that genuinely excites me. I’m truly happy to see some research groups have started working on these topics.</p> <p>I believe we’re witnessing the beginning of a fundamental shift in how decisions are made across industries. As optimization becomes accessible through technologies like LLMOPT, we may finally move away from crude heuristics toward truly optimal, data-driven decision-making.</p> <p>And that’s something worth celebrating.</p> <h2 id="recommended-resources-for-learning-more-about-ai-powered-optimization">Recommended Resources for Learning More About AI-Powered Optimization</h2> <p>For those interested in exploring this topic further, here are some valuable resources:</p> <ul> <li><a href="https://arxiv.org/abs/2410.13213">LLMOPT: Learning to Define and Solve General Optimization Problems from Scratch</a> - The original research paper</li> <li><a href="https://iclr.cc/">ICLR 2025 Conference Papers</a> - Where LLMOPT was first presented</li> </ul> ]]></content><author><name></name></author><category term="technology"/><category term="artificial intelligence"/><category term="optimization"/><category term="data science"/><category term="optimization"/><category term="AI"/><category term="large language models"/><category term="LLMOPT"/><category term="democratization"/><category term="mathematical modeling"/><category term="operations research"/><category term="decision-making"/><category term="accessibility"/><summary type="html"><![CDATA[How LLMOPT is revolutionizing access to complex optimization techniques by translating business problems into precise mathematical formulations using AI.]]></summary></entry></feed>